{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import warnings\n",
    "from tkinter import filedialog, messagebox\n",
    "import numpy as np\n",
    "import psutil\n",
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import sys\n",
    "import time\n",
    "import subprocess\n",
    "import cv2\n",
    "import leap\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_TRACKING_MODES = {\n",
    "    leap.TrackingMode.Desktop: \"Desktop\",\n",
    "    leap.TrackingMode.HMD: \"HMD\",\n",
    "    leap.TrackingMode.ScreenTop: \"ScreenTop\",\n",
    "}\n",
    "\n",
    "class GestureCapture:\n",
    "    def __init__(self, required_frames=30, save_dir=\"auth_gesture\"):\n",
    "        self.required_frames = required_frames\n",
    "        self.frames = []\n",
    "        self.capturing = False\n",
    "        self.save_dir = save_dir\n",
    "        self.tracking_mode = leap.TrackingMode.Desktop\n",
    "        \n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "    def start_capture(self):\n",
    "        \"\"\"Start capturing a single gesture.\"\"\"\n",
    "        self.capturing = True\n",
    "        self.frames = []  # Reset frames for new gesture\n",
    "        print(\"Gesture capture started!\")\n",
    "\n",
    "    def stop_capture(self):\n",
    "        \"\"\"Stop capturing and save the gesture.\"\"\"\n",
    "        self.capturing = False\n",
    "        self.save_gesture()\n",
    "        print(\"Gesture capture stopped!\")\n",
    "\n",
    "    def save_gesture(self):\n",
    "        \"\"\"Save the captured gesture as 'gesture_auth.npy' for authentication.\"\"\"\n",
    "        filename = os.path.join(self.save_dir, \"gesture_auth.npy\")\n",
    "        np.save(filename, np.array(self.frames))\n",
    "        print(f\"Authentication gesture saved with {len(self.frames)} frames.\")\n",
    "\n",
    "    def process_frame(self, event):\n",
    "        \"\"\"Extract joint positions (X, Y, Z) from Leap Motion event.\"\"\"\n",
    "        if len(event.hands) == 0:\n",
    "            return None\n",
    "\n",
    "        hand_data = []\n",
    "        for hand in event.hands:\n",
    "            for finger in hand.digits:\n",
    "                for bone in finger.bones:\n",
    "                    joint_position = [bone.prev_joint.x, bone.prev_joint.y, bone.prev_joint.z]\n",
    "                    hand_data.append(joint_position)\n",
    "        \n",
    "        return np.array(hand_data).flatten()  # Flatten the data\n",
    "\n",
    "    def render_visualization(self, event, canvas):\n",
    "        \"\"\"Render hand tracking visualization.\"\"\"\n",
    "        canvas.output_image[:, :] = 0  # Clear previous frame\n",
    "\n",
    "        if len(event.hands) == 0:\n",
    "            cv2.putText(canvas.output_image, \"No hands detected\", (50, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "        else:\n",
    "            for hand in event.hands:\n",
    "                for finger in hand.digits:\n",
    "                    for bone in finger.bones:\n",
    "                        start = canvas.get_joint_position(bone.prev_joint)\n",
    "                        end = canvas.get_joint_position(bone.next_joint)\n",
    "                        if start and end:\n",
    "                            cv2.line(canvas.output_image, start, end, (0, 255, 0), 2)\n",
    "                            cv2.circle(canvas.output_image, start, 3, (255, 0, 0), -1)\n",
    "                            cv2.circle(canvas.output_image, end, 3, (255, 0, 0), -1)\n",
    "\n",
    "        cv2.putText(canvas.output_image, f\"Frames: {len(self.frames)}\", (50, 100),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "        # cv2.imshow(\"Gesture Capture\", canvas.output_image)\n",
    "\n",
    "\n",
    "class Canvas:\n",
    "    \"\"\"Handles the visualization of the hand tracking.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.screen_size = [500, 700]\n",
    "        self.output_image = np.zeros((self.screen_size[0], self.screen_size[1], 3), np.uint8)\n",
    "\n",
    "    def get_joint_position(self, bone):\n",
    "        \"\"\"Convert Leap Motion bone coordinates to 2D screen positions.\"\"\"\n",
    "        if bone:\n",
    "            return int(bone.x + (self.screen_size[1] / 2)), int(bone.z + (self.screen_size[0] / 2))\n",
    "        return None\n",
    "\n",
    "    def set_tracking_mode(self, tracking_mode):\n",
    "        self.tracking_mode = tracking_mode\n",
    "\n",
    "    # def toggle_hands_format(self):\n",
    "    #     self.hands_format = \"Dots\" if self.hands_format == \"Skeleton\" else \"Skeleton\"\n",
    "    #     print(f\"Set hands format to {self.hands_format}\")\n",
    "\n",
    "    # def save_image(self):\n",
    "    #     filename = f\"test_image.png\"\n",
    "    #     cv2.imwrite(filename, self.output_image)\n",
    "    #     print(f\"Image saved as {filename}\")\n",
    "\n",
    "class TrackingListener(leap.Listener):\n",
    "    def __init__(self, capture, canvas):\n",
    "        self.canvas = canvas\n",
    "        self.capture = capture\n",
    "\n",
    "    # def on_connection_event(self, event):\n",
    "    #     pass\n",
    "\n",
    "    # def on_tracking_mode_event(self, event):\n",
    "    #     self.canvas.set_tracking_mode(event.current_tracking_mode)\n",
    "    #     print(f\"Tracking mode changed to {_TRACKING_MODES[event.current_tracking_mode]}\")\n",
    "\n",
    "    # def on_device_event(self, event):\n",
    "    #     try:\n",
    "    #         with event.device.open():\n",
    "    #             info = event.device.get_info()\n",
    "    #     except leap.LeapCannotOpenDeviceError:\n",
    "    #         info = event.device.get_info()\n",
    "\n",
    "    #     print(f\"Found device {info.serial}\")\n",
    "\n",
    "    def on_tracking_event(self, event):\n",
    "        \"\"\"Process Leap Motion tracking event.\"\"\"\n",
    "        self.capture.render_visualization(event, self.canvas)\n",
    "\n",
    "        if self.capture.capturing:\n",
    "            frame_data = self.capture.process_frame(event)\n",
    "            if frame_data is not None:\n",
    "                self.capture.frames.append(frame_data)\n",
    "\n",
    "            if len(self.capture.frames) >= self.capture.required_frames:\n",
    "                self.capture.stop_capture()  # Stop and save gesture automatically\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    canvas = Canvas()\n",
    "    print(\"  x: Exit\")\n",
    "    print(\"  s: Capture and save image\")\n",
    "\n",
    "    gesture_capture = GestureCapture(required_frames=100)\n",
    "    tracking_listener = TrackingListener(gesture_capture, canvas)\n",
    "    connection = leap.Connection()\n",
    "    connection.add_listener(tracking_listener)\n",
    "\n",
    "    # running = True\n",
    "\n",
    "    with connection.open():\n",
    "        print(\"Press 's' to start capturing the authentication gesture.\")\n",
    "        connection.set_tracking_mode(leap.TrackingMode.Desktop)\n",
    "        canvas.set_tracking_mode(leap.TrackingMode.Desktop)\n",
    "\n",
    "        while True:\n",
    "            cv2.imshow(\"Gesture capture\", canvas.output_image)\n",
    "            key = cv2.waitKey(1)\n",
    "\n",
    "            if key == ord('s'):\n",
    "                gesture_capture.start_capture()\n",
    "            elif not gesture_capture.capturing and len(gesture_capture.frames) > 0:\n",
    "                print(\"Gesture captured successfully. Exiting...\")\n",
    "                break  # Exit once the gesture is saved\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "class AppUnlock:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # self.model_path = 'autoencoder_model.pkl'\n",
    "        self.model_path_x = 'models/autoencoder_X.h5'\n",
    "        self.model_path_y = 'models/autoencoder_Y.h5'\n",
    "        self.model_path_z = 'models/autoencoder_Z.h5'\n",
    "\n",
    "        if (not os.path.exists(self.model_path_x)) or (not os.path.exists(self.model_path_y)) or (not os.path.exists(self.model_path_z)) :\n",
    "            print(\"No password image registered! Please run register_password.py first\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        # self.autoencoder = None\n",
    "        self.autoencoder_X = None\n",
    "        self.autoencoder_Y = None\n",
    "        self.autoencoder_Z = None\n",
    "\n",
    "        self.threshold = None\n",
    "        self.todo_app_path = 'todo_app.py'  # Path to your Todo app\n",
    "\n",
    "        self.root = tk.Tk()\n",
    "        \n",
    "        self.root.withdraw()\n",
    "        # from here\n",
    "        self.root.wm_attributes('-topmost', True)\n",
    "        # to here\n",
    "        self.monitor_todo_app()\n",
    "\n",
    "\n",
    "    def authenticate(self, autoencoder, scaler, test_data):\n",
    "        test_data = scaler.transform(test_data.reshape(1, -1))\n",
    "        reconstructed = autoencoder.predict(test_data)\n",
    "        error = np.mean(np.abs(test_data - reconstructed))\n",
    "        return error\n",
    "    \n",
    "\n",
    "    def verify_password(self, gesture_file = \"auth_gesture/gesture_auth.npy\"):\n",
    "        \"\"\"Verify password image\"\"\"\n",
    "        try:\n",
    "            self.autoencoder_X = load_model(\"models/autoencoder_X.h5\", custom_objects={\"mse\": tf.keras.losses.MeanSquaredError()})\n",
    "            self.autoencoder_Y = load_model(\"models/autoencoder_Y.h5\", custom_objects={\"mse\": tf.keras.losses.MeanSquaredError()})\n",
    "            self.autoencoder_Z = load_model(\"models/autoencoder_Z.h5\", custom_objects={\"mse\": tf.keras.losses.MeanSquaredError()})\n",
    "\n",
    "            # Load saved scalers\n",
    "            scaler_x = joblib.load(\"models/scaler_x.pkl\")\n",
    "            scaler_y = joblib.load(\"models/scaler_y.pkl\")\n",
    "            scaler_z = joblib.load(\"models/scaler_z.pkl\")\n",
    "            \n",
    "            print('Autoencoders and scalers loaded')\n",
    "\n",
    "            # if self.autoencoder is None:\n",
    "            #     with open(self.model_path, 'rb') as f:\n",
    "            #         self.autoencoder = pickle.load(f)\n",
    "            #     print('Autoencoder loaded')\n",
    "        \n",
    "            messagebox.showinfo(\"Authentication\", \"Please capture the password gesture\")\n",
    "        \n",
    "            self.root.withdraw()\n",
    "            \n",
    "            \n",
    "            main()\n",
    "            data = np.load(gesture_file)  # Load captured gesture\n",
    "            num_frames, num_features = data.shape\n",
    "\n",
    "            # Reshape and split into channels\n",
    "            num_joints = num_features // 3\n",
    "            data = data.reshape(num_frames, num_joints, 3)\n",
    "\n",
    "            X_test = data[:, :, 0]  # X coordinates\n",
    "            Y_test = data[:, :, 1]  # Y coordinates\n",
    "            Z_test = data[:, :, 2]  # Z coordinates\n",
    "\n",
    "            X_test = X_test.reshape(-1, 1)  # Ensure shape matches what scaler was trained on\n",
    "            Y_test = Y_test.reshape(-1, 1)\n",
    "            Z_test = Z_test.reshape(-1, 1)\n",
    "\n",
    "            X_test = scaler_x.transform(X_test).reshape(100, 20)  # Reshape back after scaling\n",
    "            Y_test = scaler_y.transform(Y_test).reshape(100, 20)\n",
    "            Z_test = scaler_z.transform(Z_test).reshape(100, 20)\n",
    "\n",
    "            print(\"X_test shape:\", X_test.shape)\n",
    "            print(\"Y_test shape:\", Y_test.shape)\n",
    "            print(\"Z_test shape:\", Z_test.shape)\n",
    "\n",
    "            # Flatten test data for autoencoder input\n",
    "\n",
    "            input_dim = X_test.shape[0] * X_test.shape[1]  # 100 * 20 = 600\n",
    "            X_test = X_test.reshape(1, input_dim)  # Reshape to (1, 600)\n",
    "            Y_test = Y_test.reshape(1, input_dim)\n",
    "            Z_test = Z_test.reshape(1, input_dim)\n",
    "\n",
    "            # Pass through autoencoders\n",
    "            X_reconstructed = self.autoencoder_X.predict(X_test)\n",
    "            Y_reconstructed = self.autoencoder_Y.predict(Y_test)\n",
    "            Z_reconstructed = self.autoencoder_Z.predict(Z_test)\n",
    "            \n",
    "\n",
    "            # Compute reconstruction errors (MSE) for each channel\n",
    "            error_x = mean_squared_error(X_test.flatten(), X_reconstructed.flatten())\n",
    "            error_y = mean_squared_error(Y_test.flatten(), Y_reconstructed.flatten())\n",
    "            error_z = mean_squared_error(Z_test.flatten(), Z_reconstructed.flatten())\n",
    "\n",
    "            avg_error = (error_x + error_y + error_z) / 3\n",
    "\n",
    "            print(f\"Reconstruction Errors: X={error_x:.5f}, Y={error_y:.5f}, Z={error_z:.5f}\")\n",
    "            print(f\"Average Error: {avg_error:.5f}\")\n",
    "\n",
    "            # if test_keypoints is not None:\n",
    "            #     test_sample = np.array(test_keypoints)\n",
    "            #     error = self.authenticate(self.autoencoder, scaler, test_sample)\n",
    "            #     print(error)\n",
    "\n",
    "            with open('threshold.pkl', 'rb') as t:\n",
    "                THRESHOLD = pickle.load(t)\n",
    "            \n",
    "            print(f'Threshold: {THRESHOLD}')\n",
    "            if avg_error < THRESHOLD:\n",
    "                    \n",
    "                print(\"✅ Gesture recognized! Application unlocked.\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"❌ Gesture not recognized. Access denied.\")\n",
    "                return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Verification failed: {e}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "    def monitor_todo_app(self):\n",
    "        \"\"\"Monitor Todo app process\"\"\"\n",
    "        todo_app_process = None\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # Check for todo app process\n",
    "                # found_process = None\n",
    "                for proc in psutil.process_iter(['name', 'cmdline']):\n",
    "                    if (proc.info['cmdline'] and \n",
    "                        self.todo_app_path in ' '.join(proc.info['cmdline']) and \n",
    "                        (todo_app_process is None or not todo_app_process.is_running())):\n",
    "                        found_process = proc\n",
    "                        break\n",
    "                \n",
    "                if found_process:\n",
    "                    print(\"\\nTodo App launch detected - Authentication required\")\n",
    "                    # Immediately kill the process before showing authentication\n",
    "                    found_process.kill()\n",
    "                    time.sleep(0.1)  # Small delay to ensure process is killed\n",
    "                    \n",
    "                    if self.verify_password():\n",
    "                        print(\"Access granted! Launching Todo App!\")\n",
    "                        # Launch Todo app in a new process\n",
    "                        todo_app_process = subprocess.Popen(\n",
    "                            ['python', self.todo_app_path],\n",
    "                            creationflags=subprocess.CREATE_NEW_CONSOLE\n",
    "                        )\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Access denied!\")\n",
    "                        todo_app_process = None\n",
    "                            \n",
    "            except Exception as e:\n",
    "                # print(f\"Error in monitoring: {e}\")\n",
    "                pass\n",
    "            \n",
    "            time.sleep(0.1)  # Reduce CPU usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Todo App launch detected - Authentication required\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoders and scalers loaded\n",
      "  x: Exit\n",
      "  s: Capture and save image\n",
      "Press 's' to start capturing the authentication gesture.\n",
      "Gesture capture started!\n",
      "Gesture captured successfully. Exiting...Authentication gesture saved with 100 frames.\n",
      "Gesture capture stopped!\n",
      "\n",
      "X_test shape: (100, 20)\n",
      "Y_test shape: (100, 20)\n",
      "Z_test shape: (100, 20)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "Reconstruction Errors: X=0.00133, Y=0.00061, Z=0.00764\n",
      "Average Error: 0.00319\n",
      "Threshold: 0.006087277304633347\n",
      "✅ Gesture recognized! Application unlocked.\n",
      "Access granted! Launching Todo App!\n"
     ]
    }
   ],
   "source": [
    "def AppUnlock_main():\n",
    "    app = AppUnlock()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    AppUnlock_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
