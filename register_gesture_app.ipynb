{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import leap\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "from leap import TrackingMode\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from PIL import Image\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# from tensorflow.keras.regularizers import l1\n",
    "# import psutil\n",
    "# import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_TRACKING_MODES = {\n",
    "    leap.TrackingMode.Desktop: \"Desktop\",\n",
    "    leap.TrackingMode.HMD: \"HMD\",\n",
    "    leap.TrackingMode.ScreenTop: \"ScreenTop\",\n",
    "}\n",
    "# edited\n",
    "class GestureCapture:\n",
    "    def __init__(self, required_frames=30, save_dir=\"gestures\"):\n",
    "        self.required_frames = required_frames\n",
    "        self.frames = []\n",
    "        self.capturing = False\n",
    "        self.tracking_mode = leap.TrackingMode.Desktop\n",
    "        self.save_dir = save_dir\n",
    "        self.gesture_count = 0\n",
    "        \n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "    \n",
    "    def start_capture(self):\n",
    "        self.capturing = True\n",
    "        self.frames = []  # Reset frames for new gesture\n",
    "        print(\"Gesture capture started!\")\n",
    "    \n",
    "    def stop_capture(self):\n",
    "        self.capturing = False\n",
    "        self.save_gesture()\n",
    "        print(\"Gesture capture stopped!\")\n",
    "    \n",
    "    def save_gesture(self):\n",
    "        filename = os.path.join(self.save_dir, f\"gesture_{self.gesture_count}.npy\")\n",
    "        np.save(filename, np.array(self.frames))\n",
    "        print(f\"Gesture {self.gesture_count} saved with {len(self.frames)} frames.\")\n",
    "        self.gesture_count += 1  # Increment for next gesture\n",
    "\n",
    "    def process_frame(self, event):\n",
    "        if len(event.hands) == 0:\n",
    "            return None\n",
    "\n",
    "        hand_data = []\n",
    "        for hand in event.hands:\n",
    "            for finger in hand.digits:\n",
    "                for bone in finger.bones:\n",
    "                    joint_position = [bone.prev_joint.x, bone.prev_joint.y, bone.prev_joint.z]\n",
    "                    hand_data.append(joint_position)\n",
    "        \n",
    "        return np.array(hand_data).flatten()\n",
    "    \n",
    "    def render_visualization(self, event, canvas):\n",
    "        canvas.output_image[:, :] = 0  # Clear previous frame\n",
    "        \n",
    "        if len(event.hands) == 0:\n",
    "            cv2.putText(canvas.output_image, \"No hands detected\", (50, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "        else:\n",
    "            for hand in event.hands:\n",
    "                for finger in hand.digits:\n",
    "                    for bone in finger.bones:\n",
    "                        start = canvas.get_joint_position(bone.prev_joint)\n",
    "                        end = canvas.get_joint_position(bone.next_joint)\n",
    "                        if start and end:\n",
    "                            cv2.line(canvas.output_image, start, end, (0, 255, 0), 2)\n",
    "                            cv2.circle(canvas.output_image, start, 3, (255, 0, 0), -1)\n",
    "                            cv2.circle(canvas.output_image, end, 3, (255, 0, 0), -1)\n",
    "\n",
    "        cv2.putText(canvas.output_image, f\"Frames: {len(self.frames)}\", (50, 100),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "        # cv2.imshow(\"Gesture Capture\", canvas.output_image)\n",
    "\n",
    "class TrackingListener(leap.Listener):\n",
    "    def __init__(self, capture, canvas):\n",
    "        self.capture = capture\n",
    "        self.canvas = canvas\n",
    "\n",
    "    def on_tracking_event(self, event):\n",
    "        self.capture.render_visualization(event, self.canvas)\n",
    "        \n",
    "        if self.capture.capturing:\n",
    "            frame_data = self.capture.process_frame(event)\n",
    "            if frame_data is not None:\n",
    "                self.capture.frames.append(frame_data)\n",
    "            \n",
    "            if len(self.capture.frames) >= self.capture.required_frames:\n",
    "                self.capture.stop_capture()  # Stop and save gesture\n",
    "\n",
    "\n",
    "class Canvas:\n",
    "    # edited\n",
    "    def __init__(self):\n",
    "        self.screen_size = [500, 700]\n",
    "        self.output_image = np.zeros((self.screen_size[0], self.screen_size[1], 3), np.uint8)\n",
    "\n",
    "\n",
    "    # edited\n",
    "    def get_joint_position(self, bone):\n",
    "        if bone:\n",
    "            return int(bone.x + (self.screen_size[1] / 2)), int(bone.z + (self.screen_size[0] / 2))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# edited\n",
    "def main():\n",
    "    canvas = Canvas()\n",
    "    gesture_capture = GestureCapture(required_frames=100)\n",
    "    tracking_listener = TrackingListener(gesture_capture, canvas)\n",
    "    \n",
    "    connection = leap.Connection()\n",
    "    connection.add_listener(tracking_listener)\n",
    "\n",
    "    with connection.open():\n",
    "        print(\"  x: Exit\")\n",
    "        print(\"  s: Capture and save image\")\n",
    "\n",
    "\n",
    "        while True:\n",
    "            cv2.imshow('Gesture Capturing', canvas.output_image)\n",
    "            key = cv2.waitKey(1)\n",
    "\n",
    "            if key == ord('s'):\n",
    "                gesture_capture.start_capture()\n",
    "            elif key == ord('x'):\n",
    "                break\n",
    "            \n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "class AppLock:\n",
    "    def __init__(self):\n",
    "        self.model_path_x = 'models/autoencoder_X.h5'\n",
    "        self.model_path_y = 'models/autoencoder_Y.h5'\n",
    "        self.model_path_z = 'models/autoencoder_Z.h5'\n",
    "        \n",
    "        self.autoencoder_X = None\n",
    "        self.autoencoder_Y = None\n",
    "        self.autoencoder_Z = None\n",
    "        self.show_registration_window()\n",
    "\n",
    "    def show_registration_window(self):\n",
    "        \"\"\"Show initial registration window\"\"\"\n",
    "        self.root = tk.Tk()\n",
    "        self.root.title(\"Gesture Password Registration\")\n",
    "        self.root.geometry(\"400x200\")\n",
    "        \n",
    "        label = tk.Label(\n",
    "            self.root, \n",
    "            text=\"Welcome!\\nPlease register your password gesture.\",\n",
    "            font=('Arial', 12)\n",
    "        )\n",
    "        label.pack(pady=20)\n",
    "        \n",
    "        register_btn = tk.Button(\n",
    "            self.root,\n",
    "            text=\"Please capture the password gesture\",\n",
    "            command=self.register_password,\n",
    "            font=('Arial', 11)\n",
    "        )\n",
    "        register_btn.pack(pady=20)\n",
    "        \n",
    "        self.root.mainloop()\n",
    "\n",
    "    \n",
    "    def load_and_process_gestures(self, gesture_dir):\n",
    "        gesture_files = [os.path.join(gesture_dir, f) for f in os.listdir(gesture_dir) if f.endswith(\".npy\")]\n",
    "        # Initialize lists for X, Y, Z channels\n",
    "        X_data, Y_data, Z_data = [], [], []\n",
    "        all_data = []\n",
    "\n",
    "        # Load and preprocess each gesture\n",
    "        for file in gesture_files:\n",
    "            data = np.load(file)  # Load saved gesture data\n",
    "            num_frames, num_features = data.shape  # Shape is (100, 60) -> (frames, features)\n",
    "            \n",
    "            if num_features % 3 != 0:\n",
    "                raise ValueError(f\"Unexpected shape {data.shape}, features should be a multiple of 3.\")\n",
    "\n",
    "            # Reshape into (frames, num_joints, 3)\n",
    "            num_joints = num_features // 3\n",
    "            data = data.reshape(num_frames, num_joints, 3)\n",
    "            # Ensure it has the right shape (e.g., 100 frames, 21 joints)\n",
    "            if data.shape != (100, 20, 3):  # Adjust if you expect a different shape\n",
    "                print(f\"Skipping gesture due to incorrect shape: {data.shape}\")\n",
    "                continue  # Skip malformed gesture files\n",
    "\n",
    "\n",
    "            all_data.append(data)\n",
    "\n",
    "            # Split channels\n",
    "            X_data.append(data[:, :, 0])  # X coordinates\n",
    "            Y_data.append(data[:, :, 1])  # Y coordinates\n",
    "            Z_data.append(data[:, :, 2])  # Z coordinates\n",
    "\n",
    "\n",
    "        return X_data, Y_data, Z_data, all_data\n",
    "    \n",
    "    # 2. Data Augmentation functions\n",
    "    def add_jitter(self, data, sigma=0.01):\n",
    "        return data + np.random.normal(0, sigma, data.shape)\n",
    "\n",
    "    def scale_data(self, data, scale_range=(0.9, 1.1)):\n",
    "        scale_factor = random.uniform(scale_range[0], scale_range[1])\n",
    "        return data * scale_factor\n",
    "\n",
    "    def rotate_data(self, data, angle_range=(-5, 5)):\n",
    "        angle = np.radians(random.uniform(angle_range[0], angle_range[1]))\n",
    "        cos_val, sin_val = np.cos(angle), np.sin(angle)\n",
    "        rotation_matrix = np.array([[cos_val, -sin_val, 0], [sin_val, cos_val, 0], [0, 0, 1]])\n",
    "        return np.dot(data.reshape(-1, 3), rotation_matrix).reshape(data.shape)\n",
    "\n",
    "    def time_warp(self, data, factor_range=(0.8, 1.2)):\n",
    "        \"\"\"Apply time warping by interpolating along the time axis.\"\"\"\n",
    "        factor = random.uniform(factor_range[0], factor_range[1])\n",
    "        new_length = max(1, int(data.shape[0] * factor))  # Adjust number of frames\n",
    "        \n",
    "        warped_data = np.zeros((new_length, data.shape[1], data.shape[2]))  # Same shape but different frames\n",
    "        \n",
    "        for j in range(data.shape[1]):  # Iterate over each joint\n",
    "            for k in range(data.shape[2]):  # Iterate over X, Y, Z\n",
    "                warped_data[:, j, k] = np.interp(\n",
    "                    np.linspace(0, data.shape[0] - 1, new_length),  # New frame indices\n",
    "                    np.arange(data.shape[0]),  # Original frame indices\n",
    "                    data[:, j, k]  # Original joint data\n",
    "                )\n",
    "        \n",
    "        return warped_data\n",
    "\n",
    "    def resize_sequences(self, data_list, target_length=100):\n",
    "        \"\"\"Resize sequences to a fixed number of frames.\"\"\"\n",
    "        resized_data = []\n",
    "        \n",
    "        for seq in data_list:\n",
    "            resized_seq = np.zeros((target_length, seq.shape[1], seq.shape[2]))\n",
    "            for j in range(seq.shape[1]):  # Iterate over joints\n",
    "                for k in range(seq.shape[2]):  # Iterate over X, Y, Z\n",
    "                    resized_seq[:, j, k] = np.interp(\n",
    "                        np.linspace(0, seq.shape[0] - 1, target_length),\n",
    "                        np.arange(seq.shape[0]),\n",
    "                        seq[:, j, k]\n",
    "                    )\n",
    "            resized_data.append(resized_seq)\n",
    "        \n",
    "        return np.array(resized_data)\n",
    "\n",
    "    def ensure_3d(self, data_list):\n",
    "        \"\"\"Ensure all sequences are in (frames, num_joints, 3) format.\"\"\"\n",
    "        corrected_data = []\n",
    "        \n",
    "        for seq in data_list:\n",
    "            seq = np.array(seq)\n",
    "            if len(seq.shape) == 2:  # If (frames, num_joints), add a last dimension (Z=1)\n",
    "                seq = seq.reshape(seq.shape[0], seq.shape[1], 1)  # Convert to (frames, num_joints, 1)\n",
    "            corrected_data.append(seq)\n",
    "\n",
    "        return corrected_data\n",
    "\n",
    "    def augment_data(self, data):\n",
    "        augmented_X = []\n",
    "        augmented_Y = []\n",
    "        augmented_Z = []\n",
    "\n",
    "        # Original data\n",
    "        augmented_X.append(data[:, :, 0])\n",
    "        augmented_Y.append(data[:, :, 1])\n",
    "        augmented_Z.append(data[:, :, 2])\n",
    "\n",
    "        # Augmented versions\n",
    "        for _ in range(3):  # Generate 3 augmented samples per gesture\n",
    "            aug_data = self.add_jitter(data)\n",
    "            aug_data = self.scale_data(aug_data)\n",
    "            aug_data = self.rotate_data(aug_data)\n",
    "            aug_data = self.time_warp(aug_data)\n",
    "            \n",
    "            augmented_X.append(aug_data[:, :, 0])\n",
    "            augmented_Y.append(aug_data[:, :, 1])\n",
    "            augmented_Z.append(aug_data[:, :, 2])\n",
    "\n",
    "        augmented_X = self.ensure_3d(augmented_X)\n",
    "        augmented_Y = self.ensure_3d(augmented_Y)\n",
    "        augmented_Z = self.ensure_3d(augmented_Z)\n",
    "\n",
    "\n",
    "        # Apply resizing\n",
    "        augmented_X = self.resize_sequences(augmented_X, target_length=100)\n",
    "        augmented_Y = self.resize_sequences(augmented_Y, target_length=100)\n",
    "        augmented_Z = self.resize_sequences(augmented_Z, target_length=100)\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        augmented_X = np.array(augmented_X)\n",
    "        augmented_Y = np.array(augmented_Y)\n",
    "        augmented_Z = np.array(augmented_Z)\n",
    "\n",
    "        return augmented_X, augmented_Y, augmented_Z\n",
    "\n",
    "    def build_autoencoder(self, input_dim, encoding_dim=64):\n",
    "        input_layer = Input(shape=(input_dim,))\n",
    "        # Encoder\n",
    "        encoded = Dense(128, activation=None)(input_layer)\n",
    "        encoded = LeakyReLU(alpha=0.1)(encoded)\n",
    "        encoded = BatchNormalization()(encoded)\n",
    "        encoded = Dropout(0.2)(encoded)\n",
    "        \n",
    "        encoded = Dense(encoding_dim, activation=None)(encoded)\n",
    "        encoded = LeakyReLU(alpha=0.1)(encoded)\n",
    "        \n",
    "        # Decoder\n",
    "        decoded = Dense(128, activation=None)(encoded)\n",
    "        decoded = LeakyReLU(alpha=0.1)(decoded)\n",
    "        decoded = BatchNormalization()(decoded)\n",
    "        \n",
    "        decoded = Dense(input_dim, activation=\"sigmoid\")(decoded)  # Output should match input shape\n",
    "        \n",
    "        autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "        autoencoder.compile(optimizer=\"adam\", loss=\"mse\")  # Use the registered function\n",
    "        return autoencoder\n",
    "    \n",
    "    \n",
    "    def train_autoencoder(self, X_data, Y_data, Z_data, batch_size=16, epochs=100):\n",
    "        # Load preprocessed data\n",
    "        X_data = np.load(\"preprocessed/X_data.npy\")\n",
    "        Y_data = np.load(\"preprocessed/Y_data.npy\")\n",
    "        Z_data = np.load(\"preprocessed/Z_data.npy\")\n",
    "\n",
    "        # Get input dimensions\n",
    "        input_dim = X_data.shape[1] * X_data.shape[2]  # frames * joints\n",
    "\n",
    "        # Flatten the data for training\n",
    "        X_train = X_data.reshape(X_data.shape[0], input_dim)\n",
    "        Y_train = Y_data.reshape(Y_data.shape[0], input_dim)\n",
    "        Z_train = Z_data.reshape(Z_data.shape[0], input_dim)\n",
    "\n",
    "        \"\"\"Train autoencoder on image data\"\"\"\n",
    "\n",
    "        if self.autoencoder_X is None and self.autoencoder_Y is None and self.autoencoder_Z is None:\n",
    "            self.autoencoder_X = self.build_autoencoder(input_dim)\n",
    "            self.autoencoder_Y = self.build_autoencoder(input_dim)\n",
    "            self.autoencoder_Z = self.build_autoencoder(input_dim)\n",
    "        \n",
    "        # Train the model\n",
    "        self.autoencoder_X.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, shuffle = True, verbose=1)\n",
    "        self.autoencoder_Y.fit(Y_train, Y_train, epochs=epochs, batch_size=batch_size, shuffle = True, verbose=1)\n",
    "        self.autoencoder_Z.fit(Z_train, Z_train, epochs=epochs, batch_size=batch_size, shuffle = True, verbose=1)\n",
    "\n",
    "        # Save trained models\n",
    "        self.autoencoder_X.save(self.model_path_x)\n",
    "        self.autoencoder_Y.save(self.model_path_y)\n",
    "        self.autoencoder_Z.save(self.model_path_z)\n",
    "\n",
    "        print(\"Autoencoder training completed! Models saved.\")\n",
    "\n",
    "    # def authenticate(self, autoencoder, scaler, test_data):\n",
    "    #     test_data = scaler.transform(test_data.reshape(1, -1))\n",
    "    #     reconstructed = autoencoder.predict(test_data)\n",
    "    #     error = np.mean(np.abs(test_data - reconstructed))\n",
    "    #     return error\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_threshold(self,gestures_dir):\n",
    "        autoencoder_x = load_model(\"models/autoencoder_X.h5\", custom_objects={\"mse\": tf.keras.losses.MeanSquaredError()})\n",
    "        autoencoder_y = load_model(\"models/autoencoder_Y.h5\", custom_objects={\"mse\": tf.keras.losses.MeanSquaredError()})\n",
    "        autoencoder_z = load_model(\"models/autoencoder_Z.h5\", custom_objects={\"mse\": tf.keras.losses.MeanSquaredError()})\n",
    "\n",
    "        # Load saved scalers\n",
    "        scaler_x = joblib.load(\"models/scaler_x.pkl\")\n",
    "        scaler_y = joblib.load(\"models/scaler_y.pkl\")\n",
    "        scaler_z = joblib.load(\"models/scaler_z.pkl\")\n",
    "\n",
    "\n",
    "        gesture_files = [os.path.join(gestures_dir, f) for f in os.listdir(gestures_dir) if f.endswith(\".npy\")]\n",
    "        # List to store combined reconstruction errors\n",
    "        combined_errors = []\n",
    "\n",
    "        # Loop through all gestures\n",
    "        for file in gesture_files:\n",
    "            # Load gesture data\n",
    "            gesture_data = np.load(file)  # Shape: (100, 20, 3)\n",
    "            num_frames, num_features = gesture_data.shape\n",
    "            # Reshape and split into channels\n",
    "            num_joints = num_features // 3\n",
    "            gesture_data = gesture_data.reshape(num_frames, num_joints, 3)\n",
    "\n",
    "            # Split data into X, Y, and Z channels\n",
    "            X_test = gesture_data[:, :, 0]  # Shape: (100, 20)\n",
    "            Y_test = gesture_data[:, :, 1]\n",
    "            Z_test = gesture_data[:, :, 2]\n",
    "\n",
    "\n",
    "            X_test = X_test.reshape(-1, 1)  # Ensure shape matches what scaler was trained on\n",
    "            Y_test = Y_test.reshape(-1, 1)\n",
    "            Z_test = Z_test.reshape(-1, 1)\n",
    "\n",
    "            X_test = scaler_x.transform(X_test).reshape(100, 20)  # Reshape back after scaling\n",
    "            Y_test = scaler_y.transform(Y_test).reshape(100, 20)\n",
    "            Z_test = scaler_z.transform(Z_test).reshape(100, 20)\n",
    "\n",
    "\n",
    "            input_dim = X_test.shape[0] * X_test.shape[1]  # 100 * 20 = 600\n",
    "            X_test = X_test.reshape(1, input_dim)  # Reshape to (1, 600)\n",
    "            Y_test = Y_test.reshape(1, input_dim)\n",
    "            Z_test = Z_test.reshape(1, input_dim)\n",
    "\n",
    "            # Get reconstructed data from each autoencoder\n",
    "            X_reconstructed = autoencoder_x.predict(X_test)\n",
    "            Y_reconstructed = autoencoder_y.predict(Y_test)\n",
    "            Z_reconstructed = autoencoder_z.predict(Z_test)\n",
    "\n",
    "            # Compute reconstruction errors (MSE) for each channel\n",
    "            error_x = mean_squared_error(X_test.flatten(), X_reconstructed.flatten())\n",
    "            error_y = mean_squared_error(Y_test.flatten(), Y_reconstructed.flatten())\n",
    "            error_z = mean_squared_error(Z_test.flatten(), Z_reconstructed.flatten())\n",
    "\n",
    "            # Compute final combined error (can be sum, mean, or weighted)\n",
    "            combined_error = (error_x + error_y + error_z) / 3  # Taking the average error\n",
    "            combined_errors.append(combined_error)\n",
    "\n",
    "        # Compute mean (μ) and standard deviation (σ) of combined errors\n",
    "        mu = np.mean(combined_errors)\n",
    "        sigma = np.std(combined_errors)\n",
    "\n",
    "        # Set authentication threshold\n",
    "        k = 0.25  # Adjust if needed\n",
    "        threshold = mu + 2 * sigma\n",
    "\n",
    "        with open('threshold.pkl','wb') as f:\n",
    "            pickle.dump(threshold,f)\n",
    "\n",
    "        # Print threshold details\n",
    "    \n",
    "        print(f\"Number of gestures processed: {len(combined_errors)}\")\n",
    "        print(f\"Mean Reconstruction Error: {mu:.5f}\")\n",
    "        print(f\"Standard Deviation: {sigma:.5f}\")\n",
    "        print(f\"Threshold for Authentication: {threshold:.5f}\")\n",
    "\n",
    "        return threshold\n",
    "\n",
    "    \n",
    "    def register_password(self):\n",
    "        \"\"\"Register the password gesture and train autoencoder\"\"\"\n",
    "        main()\n",
    "\n",
    "        folder_path = 'gestures'\n",
    "        try:\n",
    "            status_label = tk.Label(self.root, text=\"Training model...\", font=('Arial', 10))\n",
    "            status_label.pack(pady=10)\n",
    "            self.root.update()\n",
    "\n",
    "            X_data_list, Y_data_list, Z_data_list, all_data_list = self.load_and_process_gestures(folder_path)\n",
    "            final_X_data, final_Y_data, final_Z_data = [], [], []\n",
    "\n",
    "            for gesture_data in all_data_list:\n",
    "                aug_x, aug_y, aug_z = self.augment_data(gesture_data)\n",
    "                final_X_data.extend(aug_x)\n",
    "                final_Y_data.extend(aug_y)\n",
    "                final_Z_data.extend(aug_z)\n",
    "\n",
    "            final_X_data = np.array(final_X_data)\n",
    "            final_Y_data = np.array(final_Y_data)\n",
    "            final_Z_data = np.array(final_Z_data)\n",
    "\n",
    "            # Normalize the entire training set\n",
    "            scaler_x, scaler_y, scaler_z = MinMaxScaler(), MinMaxScaler(), MinMaxScaler()\n",
    "            final_X_data = scaler_x.fit_transform(final_X_data.reshape(-1, final_X_data.shape[-1])).reshape(final_X_data.shape)\n",
    "            final_Y_data = scaler_y.fit_transform(final_Y_data.reshape(-1, final_Y_data.shape[-1])).reshape(final_Y_data.shape)\n",
    "            final_Z_data = scaler_z.fit_transform(final_Z_data.reshape(-1, final_Z_data.shape[-1])).reshape(final_Z_data.shape)\n",
    "\n",
    "            joblib.dump(scaler_x, \"models/scaler_x.pkl\")\n",
    "            joblib.dump(scaler_y, \"models/scaler_y.pkl\")\n",
    "            joblib.dump(scaler_z, \"models/scaler_z.pkl\")\n",
    "\n",
    "            np.save(\"preprocessed/X_data.npy\", final_X_data)\n",
    "            np.save(\"preprocessed/Y_data.npy\", final_Y_data)\n",
    "            np.save(\"preprocessed/Z_data.npy\", final_Z_data)\n",
    "\n",
    "            \n",
    "            print(\"Training autoencoder...\")\n",
    "            self.train_autoencoder(final_X_data, final_Y_data, final_Z_data)\n",
    "            self.get_threshold(folder_path)\n",
    "            \n",
    "            messagebox.showinfo(\"Success\", \"Password gesture registered successfully!\")\n",
    "            print(\"Registration complete!\")\n",
    "\n",
    "            self.root.destroy()\n",
    "            \n",
    "            self.root = tk.Tk()\n",
    "            self.root.withdraw()\n",
    "            sys.exit()\n",
    "            # self.monitor_todo_app()\n",
    "            \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Registration failed: {str(e)}\")\n",
    "            print(f\"Registration failed: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  x: Exit\n",
      "  s: Capture and save image\n"
     ]
    }
   ],
   "source": [
    "def AppLock_main():\n",
    "    app_lock = AppLock()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    AppLock_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
